This subdirectory illustrates how a user may want to fetch water levels using the EDSReanaysis utilities. 
This example is designed to illustrate how a user may query all 43 yearly anual datasets and assemble the results into a single
data file. These are small test codes used to testing the code and have not been hardened for general distributon and use.
The number of output lat/lon pairs always equals the number inputted (and in the same input sort order) regardless of data missingness status

# Basic approach
The basic idea is that each year gets processed independantly. Thus a user could launch an ensemble of slurm jobs if such 
a system is available. Otherwise, a user can simply run the example scripts in a linux shell.
The simple scripts (iterate_geopoints.sh, iterate_geopoints_slurm.sh) iterate over the years and invokes the RUNME_slurm script for each year.
The RUNME_slurm script DOES NOT need to actually be passed to slurm and could be invoked on te commad line. If you do choose to launch to 
a slurm system, you will likely need to modify the #SLURM comands to suite your environment

The distinction between the "firstyear" and the remaining years in the list of years is simply to  specify column headers to be included 
in the  .csv file for the first year. This allows the user to easily 'linux cat' files together. This specification is not a requirement of the 
processing.

# Summary 

The EDSReanalysis utilities is intended to be used by users that wish to provide an arbitrary list of lat/lons and have appropriately derived weighted water level 
means to be returned. For example, given a lat/lon pair, the nearest ADCIRC irregular grid triangle (element) is identified. For example, given a lat/lon pair, 
the nearest ADCIRC irregular grid triangle (element) is identified. Then the three vetices defining that element are used to build basis functions (weights)
and the final lat/lon weighted mean is computed.

# Missingness

A non-zero water level value will be returned under the following rigorous conditions:
    1) The input lat/lon pair must be WITHIN a grid triangle. (else all nans)
    2) For each hourly time step, all three triangle vertices must have data. This can result in a
       returned lat/lon time series with blocks of nans Eg, if one vertex periodically goes dry.

# Points to consider

This code can be used to fetch water level data for the actual grid points (vertices) but some unexpected bahaviors can occur. For example, if you 
specify the actual lat/lon of a vertex, then generally, that point is shared by many triangle elements. The algorithm often cannot determine which element
is "best" and so simply chooses the first element on its list. This is fine, since the weighting algorithm will very heavily bias (weight)  the input point data
and return the actual point data.
However, if that choosen triangle contains a "dry" node or a vertex with periodic missingness, than the output water level will also have that missingness 
structure independant of the weights

# Usage 

First customize some files for your environment. Note: These simple examples are one way to utilize the codes to execute the pipeline. Many alternative invocations can be 
chosen including writing your own codes to access the functionality in utilities. The chosen example assumes the user may want to provide a list of lat/lons to process
and to fetch data from one or more of the set of 43 year reanalysis data. The reanalysis data will be extracted from the TDS hosted at RENCI. 
A few example example files containing lat/lon pair specifications are included.

The example scripts can be used to run jobs from a Linux CLI or,if available, launch jobs to a slurm managed cluster.

1) grab the codes using git. One possible way it to clone the repo
   git clone https://github.com/RENCI/EDSReanalysis.git

   let's assume the code now resides in /home/user/EDSReanalysis

   Specify some env variables: 
   export EDSREANALYSIS="/home/user/EDSReanalysis"
   export PYTHONPATH=$EDSREANALYSIS:$PYTHONPATH

2) Go to the directory: $EDSREANALYSIS/runpipeline_examples
   to make file changes. Alternatively, you can copy and execute the contents of runpipeline_examples elsewhere.
   As long as the previous environment varables wee specified, te job should run.

3) Modification of the example codes to perform a couple of runs is easy.

   a) First decide if you want to run as a shell or launch the jobs to slurm.
      iterate_geopoints.sh: For shell executions
      or
      iterate_geopoints_slurm.sh: For slurm runs. You will need to modify this file for your particular slurm environment

   b) Modify RUNME_slurm. This applies to either slurm or shell executions
      The only variable that requires changing is the specification of the file containing the lat/lon pairs. This file should be the same for all years

4) Construct an input file of lon/lat geopoints
   Several examples of files exist in $EDSREANALYSIS/testdata

  The input format is straightforward. The only mandatory entries are index,lon,lat
  The values of the index are irrelevant but it must exist. The lon/lat columns are as expected

  ,lon,lat
  0,-91.816161,30.943519
  1,-79.6725155674,32.8596518752

5) Invoke the sample job
   The test jobs are set up to run a rather small number of years and a small set of geopoints.
   Aggregates the set of years 1979, 1990, 2000, 2001, 2010 for the 200 sites in the file hsofs_200grid_lonlat.csv

   . $EDSREANALYSIS/runpipeline_examples/iterate_geopoints.sh

   Each calculation (each year)  returns 4 files: Eg for the year 1979 one gets: 1979_meta.csv,1979_data.csv,1979_data.pkl,1979_excluded_geopoints.csv

   a) 1979_data.csv is the csv file of return water levels for the input lat/lons.
      The column names are 'P' with an integer indicating order in the input file (P1,P2,,,,Pn)
  
   b) 1979_meta.csv: Returns a map of the input lat/lon points, to the internaly generated points name (Eg P1) and the grid element number
      it is associated with. Generally these should be the same for each year and do not need to be aggregated

      #Point,LON,LAT,Element (1-based)
      #P1,-64.70330600000001,32.373389,2934241
      #P2,-66.982315,44.9033,2034577
      #P3,-67.188497,44.64945,2455043 

   c) 1979_data.pkl  is the pickle formatted version of 1979_data.csv. The time indexing are datetime stamps.
      You will need to write python code to aggregate these. (Eg just do a pd.concat([df_year,df_year...],axis=0)
 
   d) 1979_excluded_geopoints.csv lists any lat/lon points that were not associated with any triangle element
      NOTE: Though listed as excluded, empty columns will be retained in the output file and in their correct placement 
      relative to the input list

6) Aggregate multiple years into a single data set
   The associated script (iterate_assemble_geopoints.sh) Shows an example of concatenating the annual files into a single data set



